
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries

@misc{richardson2016motion,
  title={Motion Capture Camera with Illuminated Status Ring},
  author={Richardson, James D and Hayes, William F and Rossmeier, Mark B},
  abstract = {A motion capture camera or motion capture system having a plurality of such cameras located around a motion capture volume where the camera includes an illuminated color-coded status ring that, under computer and/or manual control, visually communicates at a glance a status of the camera and/or system to an operator within the motion capture volume.},
  year={2016},
  month=feb # "~11",
  publisher={Google Patents},
  note={US Patent App. 14/454,248}
}

@inproceedings{ji2024exploringErgoIVR,
author = {Ji, Ruihua and Chang, Zhuang and Wang, Shuxia and Billinghurst, Mark},
title = {Exploring Effective Real-Time Ergonomic Guidance Methods for Immersive Virtual Reality Workspace},
year = {2024},
isbn = {9798400703317},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613905.3650911},
doi = {10.1145/3613905.3650911},
abstract = {Studying and working in Virtual Reality (VR) provides an immersive experience and enables free body movement for input. However, poor working posture in VR can result in discomfort and even lead to musculoskeletal problems over time. In this work, we explore how to use Mediapipe Blazepose to provide effective real-time ergonomic posture guidance methods in a virtual office environment using (1) auditory, (2) visual, and (3) combined auditory-visual cues. We do this in a within-subject design user study comparing these three conditions and a baseline condition where no guidance is provided. Our results indicate that all three guidance conditions significantly improved users’ ergonomic posture maintenance compared with the baseline condition. However, we found that the combined auditory-visual guidance is the most effective and preferred method. We also discuss multi-modal interaction methods, privacy concerns, and directions for future research.},
booktitle = {Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems},
articleno = {139},
numpages = {6},
keywords = {Ergonomic guidance, Immersive Workspace, Virtual Reality},
location = {
},
series = {CHI EA '24}
}

@book{laviola20173d,
author = {Bowman, Doug A. and Kruijff, Ernst and LaViola, Joseph J. and Poupyrev, Ivan},
title = {3D User Interfaces: Theory and Practice},
year = {2004},
isbn = {0201758679},
publisher = {Addison Wesley Longman Publishing Co., Inc.},
address = {USA}
}

@inproceedings{speicher2018selectionbased,
author = {Speicher, Marco and Feit, Anna Maria and Ziegler, Pascal and Kr\"{u}ger, Antonio},
title = {Selection-based Text Entry in Virtual Reality},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3174221},
doi = {10.1145/3173574.3174221},
abstract = {In recent years, Virtual Reality (VR) and 3D User Interfaces (3DUI) have seen a drastic increase in popularity, especially in terms of consumer-ready hardware and software. While the technology for input as well as output devices is market ready, only a few solutions for text input exist, and empirical knowledge about performance and user preferences is lacking. In this paper, we study text entry in VR by selecting characters on a virtual keyboard. We discuss the design space for assessing selection-based text entry in VR. Then, we implement six methods that span different parts of the design space and evaluate their performance and user preferences. Our results show that pointing using tracked hand-held controllers outperforms all other methods. Other methods such as head pointing can be viable alternatives depending on available resources. We summarize our findings by formulating guidelines for choosing optimal virtual keyboard text entry methods in VR.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–13},
numpages = {13},
keywords = {mid-air, pointing, task performance, text entry, user experience, virtual reality},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@InProceedings{hansberger2017gorillaarm,
author="Hansberger, Jeffrey T.
and Peng, Chao
and Mathis, Shannon L.
and Areyur Shanthakumar, Vaidyanath
and Meacham, Sarah C.
and Cao, Lizhou
and Blakely, Victoria R.",
editor="Lackey, Stephanie
and Chen, Jessie",
title="Dispelling the Gorilla Arm Syndrome: The Viability of Prolonged Gesture Interactions",
booktitle="Virtual, Augmented and Mixed Reality",
year="2017",
publisher="Springer International Publishing",
address="Cham",
pages="505--520",
abstract="The use of gestures as a way to interact with computer systems has shown promise as a natural way to interact and manipulate digital information. However, users performing mid-air gestures for even moderate periods of time experience arm fatigue and discomfort, earning its name of the gorilla arm syndrome. Based on the natural use of hands during communication, a new gesture vocabulary was created that supports the arms while the user performs the gestures. A repeated measures within subject design was conducted where participants interacted with a custom video game using 3 types of input for 30 min each, (1) keyboard, (2) mid-air gestures and (3) supported gestures. Three measures of exertion were collected, (1) time, (2) energy expenditure, and (3) perceived exertion. The newly designed supported gestures required significantly less physical and perceived effort than the mid-air gestures and required similar exertion as the keyboard condition.",
isbn="978-3-319-57987-0"
}

@inproceedings{walker2017physicalkeyboardocclusion,
author = {Walker, James and Li, Bochao and Vertanen, Keith and Kuhl, Scott},
title = {Efficient Typing on a Visually Occluded Physical Keyboard},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025783},
doi = {10.1145/3025453.3025783},
abstract = {The rise of affordable head-mounted displays (HMDs) has raised questions about how to best design user interfaces for this technology. This paper focuses on the use of HMDs for home and office applications that require substantial text input. A physical keyboard is a familiar and effective text input device in normal desktop computing. But without additional camera technology, an HMD occludes all visual feedback about a user's hand position over the keyboard. We describe a system that assists HMD users in typing on a physical keyboard. Our system has a virtual keyboard assistant that provides visual feedback inside the HMD about a user's actions on the physical keyboard. It also provides powerful automatic correction of typing errors by extending a state-of-the-art touchscreen decoder. In a study with 24 participants, we found our virtual keyboard assistant enabled users to type more accurately on a visually-occluded keyboard. We found users wearing an HMD could type at over 40 words-per-minute while obtaining an error rate of less than 5\%.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {5457–5461},
numpages = {5},
keywords = {decoder, head-mounted display, physical keyboard, text entry},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{knierim2018physicalkeyboardhands,
author = {Knierim, Pascal and Schwind, Valentin and Feit, Anna Maria and Nieuwenhuizen, Florian and Henze, Niels},
title = {Physical Keyboards in Virtual Reality: Analysis of Typing Performance and Effects of Avatar Hands},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173919},
doi = {10.1145/3173574.3173919},
abstract = {Entering text is one of the most common tasks when interacting with computing systems. Virtual Reality (VR) presents a challenge as neither the user's hands nor the physical input devices are directly visible. Hence, conventional desktop peripherals are very slow, imprecise, and cumbersome. We developed a apparatus that tracks the user's hands, and a physical keyboard, and visualize them in VR. In a text input study with 32 participants, we investigated the achievable text entry speed and the effect of hand representations and transparency on typing performance, workload, and presence. With our apparatus, experienced typists benefited from seeing their hands, and reach almost outside-VR performance. Inexperienced typists profited from semi-transparent hands, which enabled them to type just 5.6 WPM slower than with a regular desktop setup. We conclude that optimizing the visualization of hands in VR is important, especially for inexperienced typists, to enable a high typing performance.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–9},
numpages = {9},
keywords = {virtual reality, text entry, physical keyboard, hands},
location = {Montreal QC, Canada},
series = {CHI '18}
}

@inproceedings{lin2017visualizingphysicalkeyboard,
author = {Lin, Jia-Wei and Han, Ping-Hsuan and Lee, Jiun-Yu and Chen, Yang-Sheng and Chang, Ting-Wei and Chen, Kuan-Wen and Hung, Yi-Ping},
title = {Visualizing the keyboard in virtual reality for enhancing immersive experience},
year = {2017},
isbn = {9781450350150},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3102163.3102175},
doi = {10.1145/3102163.3102175},
abstract = {Recently, virtual reality (VR) becomes more and more popular and provides users an immersive experience with a head-mounted display (HMD). However, in some applications, users have to interact with physical objects while immersed in VR. With a non-see-through HMD, it is difficult to perceive visual information from the real world. Users must recall the spatial layout of the real surroundings and grope around to find the physical objects. After locating the objects, it is still inconvenient to use them without any visual feedback, which would detract the immersive experience.},
booktitle = {ACM SIGGRAPH 2017 Posters},
articleno = {35},
numpages = {2},
keywords = {keyboard, mixed reality, virtual reality},
location = {Los Angeles, California},
series = {SIGGRAPH '17}
}

@INPROCEEDINGS{xu2019pointSelectMethodsAR,
  author={Xu, Wenge and Liang, Hai-Ning and He, Anqi and Wang, Zifan},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Pointing and Selection Methods for Text Entry in Augmented Reality Head Mounted Displays}, 
  year={2019},
  volume={},
  number={},
  pages={279-288},
  abstract={Augmented reality (AR) is on the rise with consumer-level head-mounted displays (HMDs) becoming available in recent years. Text entry is an essential activity for AR systems, but it is still relatively underexplored. Although it is possible to use a physical keyboard to enter text in AR systems, it is not the most optimal and ideal way because it confines the uses to a stationary position and within indoor environments. Instead, a virtual keyboard seems more suitable. Text entry via virtual keyboards requires a pointing method and a selection mechanism. Although there exist various combinations of pointing+selection mechanisms, it is not well understood how well suited each combination is to support fast text entry speed with low error rates and positive usability (regarding workload, user experience, motion sickness, and immersion). In this research, we perform an empirical study to investigate user preference and text entry performance of four pointing methods (Controller, Head, Hand, and Hybrid) in combination with two input mechanisms (Swype and Tap). Our research represents a first systematic investigation of these eight possible combinations. Our results show that Controller outperforms all the other device-free methods in both text entry performance and user experience. However, device-free pointing methods can be usable depending on task requirements and users' preferences and physical condition.},
  keywords={Keyboards;Resists;Performance evaluation;Augmented reality;User experience;Task analysis;Handheld computers;Augmented Reality;Text Entry;User Performance;User Preference;Pointing Methods;Selection Mechanisms},
  doi={10.1109/ISMAR.2019.00026},
  ISSN={1554-7868},
  month={Oct},}

@article{dudley2018visar,
author = {Dudley, John J. and Vertanen, Keith and Kristensson, Per Ola},
title = {Fast and Precise Touch-Based Text Entry for Head-Mounted Augmented Reality with Variable Occlusion},
year = {2018},
issue_date = {December 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {25},
number = {6},
issn = {1073-0516},
url = {https://doi.org/10.1145/3232163},
doi = {10.1145/3232163},
abstract = {We present the VISAR keyboard: An augmented reality (AR) head-mounted display (HMD) system that supports text entry via a virtualised input surface. Users select keys on the virtual keyboard by imitating the process of single-hand typing on a physical touchscreen display. Our system uses a statistical decoder to infer users’ intended text and to provide error-tolerant predictions. There is also a high-precision fall-back mechanism to support users in indicating which keys should be unmodified by the auto-correction process. A unique advantage of leveraging the well-established touch input paradigm is that our system enables text entry with minimal visual clutter on the see-through display, thus preserving the user’s field-of-view. We iteratively designed and evaluated our system and show that the final iteration of the system supports a mean entry rate of 17.75wpm with a mean character error rate less than 1\%. This performance represents a 19.6\% improvement relative to the state-of-the-art baseline investigated: A gaze-then-gesture text entry technique derived from the system keyboard on the Microsoft HoloLens. Finally, we validate that the system is effective in supporting text entry in a fully mobile usage scenario likely to be encountered in industrial applications of AR HMDs.},
journal = {ACM Trans. Comput.-Hum. Interact.},
month = {dec},
articleno = {30},
numpages = {40},
keywords = {Augmented reality, text entry}
}

@article{shen2023midAirGestureTypingAR,
author = {Shen, Junxiao and Dudley, John and Kristensson, Per Ola},
title = {Fast and Robust Mid-Air Gesture Typing for AR Headsets using 3D Trajectory Decoding},
year = {2023},
issue_date = {Nov. 2023},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {29},
number = {11},
issn = {1077-2626},
url = {https://doi.org/10.1109/TVCG.2023.3320218},
doi = {10.1109/TVCG.2023.3320218},
abstract = {We present a fast mid-air gesture keyboard for head-mounted optical see-through augmented reality (OST AR) that supports users in articulating word patterns by merely moving their own physical index finger in relation to a virtual keyboard plane without a need to indirectly control a visual 2D cursor on a keyboard plane. To realize this, we introduce a novel decoding method that directly translates users' three-dimensional fingertip gestural trajectories into their intended text. We evaluate the efficacy of the system in three studies that investigate various design aspects, such as immediate efficacy, accelerated learning, and whether it is possible to maintain performance without providing visual feedback. We find that the new 3D trajectory decoding design results in significant improvements in entry rates while maintaining low error rates. In addition, we demonstrate that users can maintain their performance even without fingertip and gesture trace visualization.},
journal = {IEEE Transactions on Visualization and Computer Graphics},
month = {oct},
pages = {4622–4632},
numpages = {11}
}

@INPROCEEDINGS{kwon2020myokey,
  author={Kwon, Young D. and Shatilov, Kirill A. and Lee, Lik-Hang and Kumyol, Serkan and Lam, Kit-Yung and Yau, Yui-Pan and Hui, Pan},
  booktitle={2020 IEEE International Conference on Pervasive Computing and Communications Workshops (PerCom Workshops)}, 
  title={MyoKey: Surface Electromyography and Inertial Motion Sensing-based Text Entry in AR}, 
  year={2020},
  volume={},
  number={},
  pages={1-4},
  abstract={The seamless textual input in Augmented Reality (AR) is very challenging and essential for enabling user-friendly AR applications. Existing approaches such as speech input and vision-based gesture recognition suffer from environmental obstacles and the large default keyboard size, sacrificing the majority of the screen's real estate in AR. In this paper, we propose MyoKey, a system that enables users to effectively and unobtrusively input text in a constrained environment of AR by jointly leveraging surface Electromyography (sEMG) and Inertial Motion Unit (IMU) signals transmitted by wearable sensors on a user's forearm. MyoKey adopts a deep learning-based classifier to infer hand gestures using sEMG. In order to show the feasibility of our approach, we implement a mobile AR application using the Unity application building framework. We present novel interaction and system designs to incorporate information of hand gestures from sEMG and arm motions from IMU to provide seamless text entry solution. We demonstrate the applicability of MyoKey by conducting a series of experiments achieving the accuracy of 0.91 on identifying five gestures in real-time (Inference time: 97.43 ms).},
  keywords={Keyboards;Electromyography;Sensors;Layout;Machine learning;Muscles;Electrodes;Textual Input;Augmented Reality;EMG;IMU;Deep Learning},
  doi={10.1109/PerComWorkshops48775.2020.9156084},
  ISSN={},
  month={March},}

@inproceedings{roeber2003canesta,
author = {Roeber, Helena and Bacus, John and Tomasi, Carlo},
title = {Typing in thin air: the canesta projection keyboard - a new method of interaction with electronic devices},
year = {2003},
isbn = {1581136374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/765891.765944},
doi = {10.1145/765891.765944},
abstract = {Canesta Keyboard is a novel interface to electronic devices that consists of a projection system and a sensor module instead of the mechanical switches of a traditional keyboard. Users input text by pressing keys on a projected image of a keyboard. This paper describes the advantages and drawbacks of this interface compared to existing input methods for mobile devices in terms of data entry speed, error rate, user satisfaction and physical size as revealed through usability testing.},
booktitle = {CHI '03 Extended Abstracts on Human Factors in Computing Systems},
pages = {712–713},
numpages = {2},
location = {Ft. Lauderdale, Florida, USA},
series = {CHI EA '03}
}

@inproceedings{zhang2022typeAnywhere,
author = {Zhang, Mingrui Ray and Zhai, Shumin and Wobbrock, Jacob O.},
title = {TypeAnywhere: A QWERTY-Based Text Entry Solution for Ubiquitous Computing},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3517686},
doi = {10.1145/3491102.3517686},
abstract = {We present a QWERTY-based text entry system, TypeAnywhere, for use in off-desktop computing environments. Using a wearable device that can detect finger taps, users can leverage their touch-typing skills from physical keyboards to perform text entry on any surface. TypeAnywhere decodes typing sequences based only on finger-tap sequences without relying on tap locations. To achieve optimal decoding performance, we trained a neural language model and achieved a 1.6\% character error rate (CER) in an offline evaluation, compared to a 5.3\% CER from a traditional n-gram language model. Our user study showed that participants achieved an average performance of 70.6 WPM, or 80.4\% of their physical keyboard speed, and 1.50\% CER after 2.5 hours of practice over five days on a table surface. They also achieved 43.9 WPM and 1.37\% CER when typing on their laps. Our results demonstrate the strong potential of QWERTY typing as a ubiquitous text entry solution.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {339},
numpages = {16},
keywords = {QWERTY., Text entry, neural networks, ubiquitous computing, wearable},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{niikura2014anywhereSurfaceTouch,
author = {Niikura, Takehiro and Watanabe, Yoshihiro and Ishikawa, Masatoshi},
title = {Anywhere surface touch: utilizing any surface as an input area},
year = {2014},
isbn = {9781450327619},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2582051.2582090},
doi = {10.1145/2582051.2582090},
abstract = {The current trend towards smaller and smaller mobile devices may cause considerable difficulties in using them. In this paper, we propose an interface called Anywhere Surface Touch, which allows any flat or curved surface in a real environment to be used as an input area. The interface uses only a single small camera and a contact microphone to recognize several kinds of interaction between the fingers of the user and the surface. The system recognizes which fingers are interacting and in which direction the fingers are moving. Additionally, the fusion of vision and sound allows the system to distinguish the contact conditions between the fingers and the surface. Evaluation experiments showed that users became accustomed to our system quickly, soon being able to perform input operations on various surfaces.},
booktitle = {Proceedings of the 5th Augmented Human International Conference},
articleno = {39},
numpages = {8},
keywords = {vision-based UI, touch interface, mobile devices, interaction techniques, high-speed camera},
location = {Kobe, Japan},
series = {AH '14}
}

@article{shi2018toast,
author = {Shi, Weinan and Yu, Chun and Yi, Xin and Li, Zhen and Shi, Yuanchun},
title = {TOAST: Ten-Finger Eyes-Free Typing on Touchable Surfaces},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3191765},
doi = {10.1145/3191765},
abstract = {Touch typing on flat surfaces (e.g. interactive tabletop) is challenging due to lack of tactile feedback and hand drifting. In this paper, we present TOAST, an eyes-free keyboard technique for enabling efficient touch typing on touch-sensitive surfaces. We first formalized the problem of keyboard parameter (e.g. location and size) estimation based on users' typing data. Through a user study, we then examined users' eyes-free touch typing behavior on an interactive tabletop with only asterisk feedback. We fitted the keyboard model to the typing data, results suggested that the model parameters (keyboard location and size) changed not only between different users, but also within the same user along with time. Based on the results, we proposed a Markov-Bayesian algorithm for input prediction, which considers the relative location between successive touch points within each hand respectively. Simulation results showed that based on the pooled data from all users, this model improved the top-1 accuracy of the classical statistical decoding algorithm from 86.2\% to 92.1\%. In a second user study, we further improved TOAST with dynamical model parameter adaptation, and evaluated users' text entry performance with TOAST using realistic text entry tasks. Participants reached a pick-up speed of 41.4 WPM with a character-level error rate of 0.6\%. And with less than 10 minutes of practice, they reached 44.6 WPM without sacrificing accuracy. Participants' subjective feedback also indicated that TOAST offered a natural and efficient typing experience.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {mar},
articleno = {33},
numpages = {23},
keywords = {Text entry, adaptive keyboard, eyes-free, ten-finger}
}

@article{sulaiman2008tangisoft,
  title={TangiSoft: Designing a Tangible Direct-Touch Tabletop Keyboard},
  author={Sulaiman, A and Olivier, Patrick and Heslop, Philip and others},
  journal={School of Computing Science Technical Report Series},
  year={2008},
  publisher={Newcastle University}
}

@inproceedings{lee2003arkb,
  title={ARKB: 3D vision-based Augmented Reality Keyboard.},
  author={Lee, Minkyung and Woo, Woontack and others},
  booktitle={ICAT},
  year={2003}
}

@INPROCEEDINGS{dudley2019performanceEnvelopes,
  author={Dudley, John and Benko, Hrvoje and Wigdor, Daniel and Kristensson, Per Ola},
  booktitle={2019 IEEE International Symposium on Mixed and Augmented Reality (ISMAR)}, 
  title={Performance Envelopes of Virtual Keyboard Text Input Strategies in Virtual Reality}, 
  year={2019},
  volume={},
  number={},
  pages={289-300},
  abstract={Virtual and Augmented Reality deliver engaging interaction experiences that can transport and extend the capabilities of the user. To ensure these paradigms are more broadly usable and effective, however, it is necessary to also deliver many of the conventional functions of a smartphone or personal computer. It remains unclear how conventional input tasks, such as text entry, can best be translated into virtual and augmented reality. In this paper we examine the performance potential of four alternative text entry strategies in virtual reality (VR). These four strategies are selected to provide full coverage of two fundamental design dimensions: i) physical surface association; and ii) number of engaged fingers. Specifically, we examine typing with index fingers on a surface and in mid-air and typing using all ten fingers on a surface and in mid-air. The central objective is to evaluate the human performance potential of these four typing strategies without being constrained by current tracking and statistical text decoding limitations. To this end we introduce an auto-correction simulator that uses knowledge of the stimulus to emulate statistical text decoding within constrained experimental parameters and use high-precision motion tracking hardware to visualise and detect fingertip interactions. We find that alignment of the virtual keyboard with a physical surface delivers significantly faster entry rates over a mid-air keyboard. Also, users overwhelmingly fail to effectively engage all ten fingers in mid-air typing, resulting in slower entry rates and higher error rates compared to just using two index fingers. In addition to identifying the envelopes of human performance for the four strategies investigated, we also provide a detailed analysis of the underlying features that distinguish each strategy in terms of its performance and behaviour.},
  keywords={Keyboards;Tracking;Layout;Decoding;Augmented reality;Performance evaluation;Indexes;virtual reality;text entry;head mounted display},
  doi={10.1109/ISMAR.2019.00027},
  ISSN={1554-7868},
  month={Oct},}

@inproceedings{higuchi2013arTypingMobile,
author = {Higuchi, Masakazu and Komuro, Takashi},
title = {AR typing interface for mobile devices},
year = {2013},
isbn = {9781450326483},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2541831.2541847},
doi = {10.1145/2541831.2541847},
abstract = {We propose a new user interface system for mobile devices. By using augmented reality (AR) technology, the system overlays virtual objects on real images captured by a camera attached to the back of a mobile device, and the user can operate the mobile device by manipulating the virtual objects with his/her hand in the space behind the mobile device. This system allows the user to operate the device in a wide three-dimensional space and to select small objects easily. Also, the AR technology provides the user with a sense of reality in operating the device. We developed a typing application using our system and verified the effectiveness by user studies. The results showed that more than half of the subjects felt that the operation area of the proposed system is larger than that of a smartphone and that both AR and unfixed key-plane are effective for improving typing speed.},
booktitle = {Proceedings of the 12th International Conference on Mobile and Ubiquitous Multimedia},
articleno = {14},
numpages = {8},
keywords = {augmented reality, finger tracking, virtual keyboard},
location = {Lule\r{a}, Sweden},
series = {MUM '13}
}

@inproceedings{Streli_2022_TapType,
author = {Streli, Paul and Jiang, Jiaxi and Fender, Andreas Rene and Meier, Manuel and Romat, Hugo and Holz, Christian},
title = {TapType: Ten-finger text entry on everyday surfaces via Bayesian inference},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3501878},
doi = {10.1145/3491102.3501878},
abstract = {Despite the advent of touchscreens, typing on physical keyboards remains most efficient for entering text, because users can leverage all fingers across a full-size keyboard for convenient typing. As users increasingly type on the go, text input on mobile and wearable devices has had to compromise on full-size typing. In this paper, we present TapType, a mobile text entry system for full-size typing on passive surfaces—without an actual keyboard. From the inertial sensors inside a band on either wrist, TapType decodes and relates surface taps to a traditional QWERTY keyboard layout. The key novelty of our method is to predict the most likely character sequences by fusing the finger probabilities from our Bayesian neural network classifier with the characters’ prior probabilities from an n-gram language model. In our online evaluation, participants on average typed 19 words per minute with a character error rate of 0.6\% after 30 minutes of training. Expert typists thereby consistently achieved more than 25&nbsp;WPM at a similar error rate. We demonstrate applications of TapType in mobile use around smartphones and tablets, as a complement to interaction in situated Mixed Reality outside visual control, and as an eyes-free mobile text input method using an audio feedback-only interface.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {497},
numpages = {16},
keywords = {virtual reality, n-gram language model, mobile text entry, invisible interfaces, Bayesian neural network, Bayesian inference},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{hirai2021textEntryHapticsRealObjects,
author = {Hirai, Rio and Ikeda, Ryo and Shizuki, Buntarou},
title = {Preliminary Investigation of Text Entry Method with Haptic Feedback from Real Object Surfaces Estimated Using Hand Tracking on HMD},
year = {2021},
isbn = {9781450382038},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3429360.3468194},
doi = {10.1145/3429360.3468194},
abstract = {In virtual reality systems, users enter text by selecting virtual keys with the fingers. A virtual keyboard is displayed in mid-air and thus does not provide haptic feedback. To address this problem, we present a text entry method that uses the surfaces of real objects around the user to provide haptic feedback. A real object surface touched a hand is recognized using the position and posture of the hand acquired by a hand-tracking sensor on a head-mounted display; a virtual keyboard is placed on that surface to provide haptic feedback. We performed a pilot study to compare text entry performance when the virtual keyboard was placed in mid-air, on a wall, on a desk, and on the user’s thighs. The result shows that each virtual keyboard placement and the presence/absence of haptic feedback did not affect input performance.},
booktitle = {Proceedings of the Asian CHI Symposium 2021},
pages = {129–131},
numpages = {3},
keywords = {on-world interaction, soft keyboard, touch interaction, virtual reality},
location = {Yokohama, Japan},
series = {Asian CHI '21}
}

@INPROCEEDINGS{Zhaoyuan2015HapticKeyClick,
  author={Zhaoyuan Ma and Edge, Darren and Findlater, Leah and Tan, Hong Z.},
  booktitle={2015 IEEE World Haptics Conference (WHC)}, 
  title={Haptic keyclick feedback improves typing speed and reduces typing errors on a flat keyboard}, 
  year={2015},
  volume={},
  number={},
  pages={220-227},
  abstract={The present study used a flat keyboard without moving keys and enabled with haptic keyclick feedback to examine the effect of haptic keyclick feedback on touch typing performance. We investigated, with well-controlled stimuli and a within-participant design, how haptic keyclick feedback might improve typing performance in terms of typing speed, typing efficiency and typing errors. Of the three kinds of haptic feedback we tested, all increased typing speed and decreased typing errors compared to a condition without haptic feedback. We did not find significant differences among the types of haptic feedback. We also found that auditory keyclick feedback alone is not as effective as haptic keyclick feedback, and the addition of auditory feedback to haptic feedback does not lead to any significant improvement in typing performance. We also learned that global haptic keyclick feedback simulated through local keyclick feedback on each key (as opposed to haptic feedback all over the keyboard) might have the additional and unexpected benefit of helping a typist to locate keys on a keyboard. Furthermore, the participants preferred auditory or haptic keyclick feedback to no feedback, and haptic feedback restricted to the typing finger alone is preferred to that over a larger area of the keyboard.},
  keywords={Haptic interfaces;Keyboards;Error analysis;Thumb;Visualization;Headphones},
  doi={10.1109/WHC.2015.7177717},
  ISSN={},
  month={June},}

@article{Gu_2020_QwertyRing,
author = {Gu, Yizheng and Yu, Chun and Li, Zhipeng and Li, Zhaoheng and Wei, Xiaoying and Shi, Yuanchun},
title = {QwertyRing: Text Entry on Physical Surfaces Using a Ring},
year = {2020},
issue_date = {December 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {4},
number = {4},
url = {https://doi.org/10.1145/3432204},
doi = {10.1145/3432204},
abstract = {The software keyboard is widely used on digital devices such as smartphones, computers, and tablets. The software keyboard operates via touch, which is efficient, convenient, and familiar to users. However, some emerging technology devices such as AR/VR headsets and smart TVs do not support touch-based text entry. In this paper, we present QwertyRing, a technique that supports text entry on physical surfaces using an IMU (Inertial Measurement Unit) ring. Users wear the ring on the middle phalanx of the index finger and type on any desk-like surface, as if there is a QWERTY keyboard on the surface. While typing, users do not focus on monitoring the hand motions. They receive text feedback on a separate screen, e.g., an AR/VR headset or a digital device display, such as a computer monitor. The basic idea of QwertyRing is to detect touch events and predict users' desired words by the orientation of the IMU ring. We evaluate the performance of QwertyRing through a five-day user study. Participants achieved a speed of 13.74 WPM in the first 40 minutes and reached 20.59 WPM at the end. The speed outperforms other ring-based techniques [24, 30, 45, 68] and is 86.48\% of the speed of typing on a smartphone with an index finger. The results show that QwertyRing enables efficient touch-based text entry on physical surfaces.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {dec},
articleno = {128},
numpages = {29},
keywords = {smart ring, text entry, touch input}
}

@article{Kung2021UsabilityStudy,
title = {Usability study of multiple vibrotactile feedback stimuli in an entire virtual keyboard input},
journal = {Applied Ergonomics},
volume = {90},
pages = {103270},
year = {2021},
issn = {0003-6870},
doi = {https://doi.org/10.1016/j.apergo.2020.103270},
url = {https://www.sciencedirect.com/science/article/pii/S0003687020302192},
author = {Chia-Hsuan Kung and Tzu-Chieh Hsieh and Shana Smith},
keywords = {Virtual keyboard, Touchscreen, Usability, Multiple vibrotactile feedback, Discriminable vibration patterns},
abstract = {With advances in information technology, people spend more time on touchscreen-based virtual keyboards than physical keyboards. However, typing on touchscreens usually lacks informative tactile feedback and anchoring references to locate the right keys, and thus requires more visual attention. Most prior tactile keyboard research used single stimulus pattern, which was not enough to recognize different keys. The purpose of this study was to investigate the usability of multiple vibrotactile feedback patterns in an entire virtual QWERTY keyboard input. A set of highly discriminable vibration patterns was designed and associated with different regions of a virtual keyboard to help users to locate the right keys. However, the number of stimulus patterns might also affect the typing performance. A user study was conducted to evaluate the effectiveness of the multiple vibrotactile feedback. The results showed that an appropriate number of stimulus patterns provided higher typing speed, higher typing efficiency, and lower error rate.}
}

@inproceedings{Gupta_et_al_2019,
author = {Gupta, Aakar and Ji, Cheng and Yeo, Hui-Shyong and Quigley, Aaron and Vogel, Daniel},
title = {RotoSwype: Word-Gesture Typing using a Ring},
year = {2019},
isbn = {9781450359702},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290605.3300244},
doi = {10.1145/3290605.3300244},
abstract = {We propose RotoSwype, a technique for word-gesture typing using the orientation of a ring worn on the index finger. RotoSwype enables one-handed text-input without encumbering the hand with a device, a desirable quality in many scenarios, including virtual or augmented reality. The method is evaluated using two arm positions: with the hand raised up with the palm parallel to the ground; and with the hand resting at the side with the palm facing the body. A five-day study finds both hand positions achieved speeds of at least 14 words-per-minute (WPM) with uncorrected error rates near 1\%, outperforming previous comparable techniques.},
booktitle = {Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–12},
numpages = {12},
keywords = {interaction techniques, controlled experiments},
location = {Glasgow, Scotland Uk},
series = {CHI '19}
}

@ARTICLE{Dudley_et_al_2023,
  author={Dudley, John J. and Zheng, Jingyao and Gupta, Aakar and Benko, Hrvoje and Longest, Matt and Wang, Robert and Kristensson, Per Ola},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Evaluating the Performance of Hand-Based Probabilistic Text Input Methods on a Mid-Air Virtual Qwerty Keyboard}, 
  year={2023},
  volume={29},
  number={11},
  pages={4567-4577},
  abstract={Integrated hand-tracking on modern virtual reality (VR) headsets can be readily exploited to deliver mid-air virtual input surfaces for text entry. These virtual input surfaces can closely replicate the experience of typing on a Qwerty keyboard on a physical touchscreen, thereby allowing users to leverage their pre-existing typing skills. However, the lack of passive haptic feedback, unconstrained user motion, and potential tracking inaccuracies or observability issues encountered in this interaction setting typically degrades the accuracy of user articulations. We present a comprehensive exploration of error-tolerant probabilistic hand-based input methods to support effective text input on a mid-air virtual Qwerty keyboard. Over three user studies we examine the performance potential of hand-based text input under both gesture and touch typing paradigms. We demonstrate typical entry rates in the range of 20 to 30 wpm and average peak entry rates of 40 to 45 wpm.},
  keywords={Keyboards;Probabilistic logic;Touch sensitive screens;Headphones;Decoding;Protocols;Performance evaluation;Virtual reality;text entry},
  doi={10.1109/TVCG.2023.3320238},
  ISSN={1941-0506},
  month={Nov},}

@inproceedings{Son_et_al_2019,
author = {Son, Jeongmin and Ahn, Sunggeun and Kim, Sunbum and Lee, Geehyuk},
title = {
},
year = {2019},
isbn = {9781450359719},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3290607.3312926},
doi = {10.1145/3290607.3312926},
abstract = {Two-Thumb Touchpad Typing (4T) using hand-held controllers is one of the common text entry techniques in Virtual Reality (VR). However, its performance is far below that of two-thumb typing on a smartphone. We explored the possibility of improving its performance focusing on the following two factors: the visual feedback of hovering thumbs and the grip stability of the controllers. We examined the effects of these two factors on the performance of 4T in VR in user experiments. Their results show that hover feedback had a significant main effect on the 4T performance, but grip stability did not. We then investigated the achievable performance of the final 4T design in a longitudinal study, and its results show that users could achieve a typing speed over 30 words per minute after two hours of practice.},
booktitle = {Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems},
pages = {1–6},
numpages = {6},
keywords = {virtual reality, two-thumb touchpad typing, text entry},
location = {Glasgow, Scotland Uk},
series = {CHI EA '19}
}

@INPROCEEDINGS{Jiang_Weng_2020,
  author={Jiang, Haiyan and Weng, Dongdong},
  booktitle={2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={HiPad: Text entry for Head-Mounted Displays Using Circular Touchpad}, 
  year={2020},
  volume={},
  number={},
  pages={692-703},
  abstract={Text entry in virtual reality (VR) is currently a common activity and a challenging problem. In this paper, we introduce HiPad, leveraging a circular touchpad with a circular virtual keyboard, to support the one-hand text entry in mobile head-mounted displays (HMDs). The design of HiPad’s layout is based on a circle and a square with rounded corners, where the outer circle is subdivided into six keys’ regions containing letters. This technique input text by a common hand-held controller with a circular touchpad for HMDs and disambiguates the word based on the sequence of keys pressed by the user. In our first study, three potential layouts are considered and evaluated, leading to the design containing six keys. By analyzing the touch behavior of users, we optimize the 6-keys layout and conduct the second study, showing that the optimized layout has better performance. Then the third study is conducted to evaluate the performance of 6-keys HiPad with VE-layout and TP-layout and to study the learning curves. The results show that novices can achieve 13.57 Words per Minute (WPM) with VE-layout and 11.60 WPM with TP-layout and the speeds increase by 74.42% for VE-layout users and by 81.53% for TP-layout users through a short 60-phrase training.},
  keywords={Layout;Keyboards;Training;Virtual reality;Visualization;Sensors;Conferences;Human-centered computing;Human computer interaction;Interaction paradigms;Virtual reality;Human-centered computing;Interaction techniques;Text input},
  doi={10.1109/VR46266.2020.00092},
  ISSN={2642-5254},
  month={March},}

@ARTICLE{Yu_et_al_2018,
  author={Yu, Difeng and Fan, Kaixuan and Zhang, Heng and Monteiro, Diego and Xu, Wenge and Liang, Hai-Ning},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={PizzaText: Text Entry for Virtual Reality Systems Using Dual Thumbsticks}, 
  year={2018},
  volume={24},
  number={11},
  pages={2927-2935},
  keywords={Layout;Keyboards;Games;Training;Virtual reality;Google;Fans;Virtual reality;text entry;game controller;dual-joystick input;selection keyboard;circular keyboard layout},
  doi={10.1109/TVCG.2018.2868581}}

@inproceedings{Palmeira_et_al_2023,
author = {Palmeira, Eduardo G. Q. and Campos, Alexandre and Moraes, \'{I}gor A. and de Siqueira, Alexandre G. and Ferreira, Marcelo G. G.},
title = {Quantifying the 'Gorilla Arm' Effect in a Virtual Reality Text Entry Task via Ray-Casting: A Preliminary Single-Subject Study},
year = {2024},
isbn = {9798400709432},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3625008.3625046},
doi = {10.1145/3625008.3625046},
abstract = {Inputting text is a fundamental task users perform in immersive virtual reality (VR) environments. This preliminary study quantifies the ‘gorilla arm’ effect regarding the required torque in the upper limb joints of an individual during a VR text entry task via ray-casting. This type of fatigue can be caused by the weight of the users’ arms, depending on the arm pose maintained during a task. We used motion capture technology to compare the required arm joints’ torque in two typical arm poses during VR text entry tasks. The results suggest that extending the arm higher and in front of the body towards the virtual keyboard causes about 3.77 times more net torque on the shoulder joint and 2.00 times more on the elbow joint, compared to when the arm is in a more relaxed and lowered position. These findings can support future VR text entry studies concerned with physical ergonomics.},
booktitle = {Proceedings of the 25th Symposium on Virtual and Augmented Reality},
pages = {274–278},
numpages = {5},
keywords = {biomechanics, gorilla arm syndrome, text input, virtual reality},
location = {<conf-loc>, <city>Rio Grande</city>, <country>Brazil</country>, </conf-loc>},
series = {SVR '23}
}

@inproceedings{Markussen_2014_Vulture,
author = {Markussen, Anders and Jakobsen, Mikkel R\o{}nne and Hornb\ae{}k, Kasper},
title = {Vulture: a mid-air word-gesture keyboard},
year = {2014},
isbn = {9781450324731},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2556288.2556964},
doi = {10.1145/2556288.2556964},
abstract = {Word-gesture keyboards enable fast text entry by letting users draw the shape of a word on the input surface. Such keyboards have been used extensively for touch devices, but not in mid-air, even though their fluent gestural input seems well suited for this modality. We present Vulture, a word-gesture keyboard for mid-air operation. Vulture adapts touch based word-gesture algorithms to work in mid-air, projects users' movement onto the display, and uses pinch as a word delimiter. A first 10-session study suggests text-entry rates of 20.6 Words Per Minute (WPM) and finds hand-movement speed to be the primary predictor of WPM. A second study shows that with training on a few phrases, participants do 28.1 WPM, 59\% of the text-entry rate of direct touch input. Participants' recall of trained gestures in mid-air was low, suggesting that visual feedback is important but also limits performance. Based on data from the studies, we discuss improvements to Vulture and some alternative designs for mid-air text entry.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1073–1082},
numpages = {10},
keywords = {word-gesture keyboard, text entry, shape writing, mid-air interaction, in-air interaction, freehand interaction},
location = {Toronto, Ontario, Canada},
series = {CHI '14}
}

@inproceedings{Kim_2023_STAR,
author = {Kim, Taejun and Karlson, Amy and Gupta, Aakar and Grossman, Tovi and Wu, Jason and Abtahi, Parastoo and Collins, Christopher and Glueck, Michael and Surale, Hemant Bhaskar},
title = {STAR: Smartphone-analogous Typing in Augmented Reality},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606803},
doi = {10.1145/3586183.3606803},
abstract = {While text entry is an essential and frequent task in Augmented Reality (AR) applications, devising an efficient and easy-to-use text entry method for AR remains an open challenge. This research presents STAR, a smartphone-analogous AR text entry technique that leverages a user’s familiarity with smartphone two-thumb typing. With STAR, a user performs thumb typing on a virtual QWERTY keyboard that is overlain on the skin of their hands. During an evaluation study of STAR, participants achieved a mean typing speed of 21.9 WPM (i.e., 56\% of their smartphone typing speed), and a mean error rate of 0.3\% after 30 minutes of practice. We further analyze the major factors implicated in the performance gap between STAR and smartphone typing, and discuss ways this gap could be narrowed.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {116},
numpages = {13},
keywords = {Augmented Reality, Head-Mounted Display, QWERTY Keyboard, Smartphone Typing, Text Entry},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{crabtree_2016_dayInthelife,
author = {Crabtree, Andy and Tolmie, Peter},
title = {A Day in the Life of Things in the Home},
year = {2016},
isbn = {9781450335928},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2818048.2819954},
doi = {10.1145/2818048.2819954},
abstract = {This paper is about human interaction with things in the home. It is of potential relevance to developers of the Internet of Things (IoT), but it is not a technological paper. Rather, it presents a preliminary observational study of a day in a life of things in the home. The study was done out of curiosity - to see, given the emphasis on 'things' in the IoT, what mundane interaction with things looks like and is about. The results draw attention to the sheer scale of interaction with things, key areas of domestic activity in which interaction is embedded, and what it is about domestic life that gives data about interaction its sense. Each of these issues raises possibilities and challenges for IoT development in the home.},
booktitle = {Proceedings of the 19th ACM Conference on Computer-Supported Cooperative Work \& Social Computing},
pages = {1738–1750},
numpages = {13},
keywords = {Ethnography, Internet of Things., domestic environment, mundane interaction with things},
location = {San Francisco, California, USA},
series = {CSCW '16}
}

@inproceedings{Cheng_2010_icon,
author = {Cheng, Kai-Yin and Liang, Rong-Hao and Chen, Bing-Yu and Laing, Rung-Huei and Kuo, Sy-Yen},
title = {iCon: utilizing everyday objects as additional, auxiliary and instant tabletop controllers},
year = {2010},
isbn = {9781605589299},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1753326.1753499},
doi = {10.1145/1753326.1753499},
abstract = {This work describes a novel approach to utilizing everyday objects of users as additional, auxiliary, and instant tabletop controllers. Based on this approach, a prototype platform, called iCon, is developed to explore the possible design. Field studies and user studies reveal that utilizing everyday objects such as auxiliary input devices might be appropriate under a multi-task scenario. User studies further demonstrate that daily objects can generally be applied in low precision circumstances, low engagement with selected objects, and medium-to-high frequency of use. The proposed approach allows users to interact with computers while not altering their original work environments.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1155–1164},
numpages = {10},
keywords = {everyday object, tabletop controller, tangible user interface},
location = {Atlanta, Georgia, USA},
series = {CHI '10}
}

@article{Greenslade_2023_PropsinARGames,
author = {Greenslade, Mac and Clark, Adrian and Lukosch, Stephan},
title = {Using Everyday Objects as Props for Virtual Objects in First Person Augmented Reality Games: An Elicitation Study},
year = {2023},
issue_date = {November 2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {7},
number = {CHI PLAY},
url = {https://doi.org/10.1145/3611052},
doi = {10.1145/3611052},
abstract = {In this paper, we present an elicitation study which explores how people use common household objects as props to control virtual objects in augmented reality first-person perspective games. 24 participants were asked to select items from a range of common household objects to use as controllers for three different types of virtual object: a sword, shield, and crossbow. Participants completed short gameplay tasks using their selected items and rated the AR experience using the Augmented Reality Immersion (ARI) questionnaire. Results found no strong consensus linking any specific household object to any virtual object across our test group and, in addition, those who chose the most commonly selected object for each task did not have significantly higher scores on the ARI questionnaire compared to those who did not. A short post-experiment interview indicated a few key factors that were important to participants when selecting their household object, such as shape, size, grip feel and weight distribution. Based on our findings we recommend that developers provide the ability for users to choose which household objects to use as props based on the user's own preferences, and that they design intuitive ways for users to interact with virtual objects.},
journal = {Proc. ACM Hum.-Comput. Interact.},
month = {oct},
articleno = {406},
numpages = {20},
keywords = {passive haptics, games, everyday objects, elicitation study}
}

@inproceedings{Fang_2023_VRHapticAtHome,
author = {Fang, Cathy Mengying and Suzuki, Ryo and Leithinger, Daniel},
title = {VR Haptics at Home: Repurposing Everyday Objects and Environment for Casual and On-Demand VR Haptic Experiences},
year = {2023},
isbn = {9781450394222},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544549.3585871},
doi = {10.1145/3544549.3585871},
abstract = {This paper introduces VR Haptics at Home, a method of repurposing everyday objects in the home to provide casual and on-demand haptic experiences. Current VR haptic devices are often expensive, complex, and unreliable, which limits the opportunities for rich haptic experiences outside research labs. In contrast, we envision that, by repurposing everyday objects as passive haptics props, we can create engaging VR experiences for casual uses with minimal cost and setup. To explore and evaluate this idea, we conducted an in-the-wild study with eight participants, in which they used our proof-of-concept system to turn their surrounding objects such as chairs, tables, and pillows at their own homes into haptic props. The study results show that our method can be adapted to different homes and environments, enabling more engaging VR experiences without the need for complex setup process. Based on our findings, we propose a possible design space to showcase the potential for future investigation.},
booktitle = {Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {312},
numpages = {7},
keywords = {Interaction Techniques, Passive Haptics, Virtual Reality},
location = {Hamburg, Germany},
series = {CHI EA '23}
}

@inproceedings{Henderson_2008_Controls,
author = {Henderson, Steven J. and Feiner, Steven},
title = {Opportunistic controls: leveraging natural affordances as tangible user interfaces for augmented reality},
year = {2008},
isbn = {9781595939517},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1450579.1450625},
doi = {10.1145/1450579.1450625},
abstract = {We present Opportunistic Controls, a class of user interaction techniques for augmented reality (AR) applications that support gesturing on, and receiving feedback from, otherwise unused affordances already present in the domain environment. Opportunistic Controls leverage characteristics of these affordances to provide passive haptics that ease gesture input, simplify gesture recognition, and provide tangible feedback to the user. 3D widgets are tightly coupled with affordances to provide visual feedback and hints about the functionality of the control. For example, a set of buttons is mapped to existing tactile features on domain objects. We describe examples of Opportunistic Controls that we have designed and implemented using optical marker tracking, combined with appearance-based gesture recognition. We present the results of a user study in which participants performed a simulated maintenance inspection of an aircraft engine using a set of virtual buttons implemented both as Opportunistic Controls and using simpler passive haptics. Opportunistic Controls allowed participants to complete their tasks significantly faster and were preferred over the baseline technique.},
booktitle = {Proceedings of the 2008 ACM Symposium on Virtual Reality Software and Technology},
pages = {211–218},
numpages = {8},
keywords = {tangible user interfaces, selection metaphor, augmented reality, 3D interaction},
location = {Bordeaux, France},
series = {VRST '08}
}

@inproceedings{Roudaut_2011_TouchInput,
author = {Roudaut, Anne and Pohl, Henning and Baudisch, Patrick},
title = {Touch input on curved surfaces},
year = {2011},
isbn = {9781450302289},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1978942.1979094},
doi = {10.1145/1978942.1979094},
abstract = {Advances in sensing technology are currently bringing touch input to non-planar surfaces, ranging from spherical touch screens to prototypes the size and shape of a ping-pong ball. To help interface designers create usable interfaces on such devices, we determine how touch surface curvature affects targeting. We present a user study in which participants acquired targets on surfaces of different curvature and at locations of different slope. We find that surface convexity increases pointing accuracy, and in particular reduces the offset between the input point perceived by users and the input point sensed by the device. Concave surfaces, in contrast, are subject to larger error offsets. This is likely caused by how concave surfaces hug the user's finger, thus resulting in a larger contact area. The effect of slope on targeting, in contrast, is unexpected at first sight. Some targets located downhill from the user's perspective are subject to error offsets in the opposite direction from all others. This appears to be caused by participants acquiring these targets using a different finger posture that lets them monitor the position of their fingers more effectively.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {1011–1020},
numpages = {10},
keywords = {curved, flexible, form factor, industrial design, non-planar, pointing, shape of device, targeting, touch},
location = {Vancouver, BC, Canada},
series = {CHI '11}
}

@inproceedings{Drogemuller_2021_TurningEverdayObjects,
author = {Drogemuller, Adam and Walsh, James and Smith, Ross T. and Adcock, Matt and Thomas, Bruce H},
title = {Turning everyday objects into passive tangible controllers},
year = {2021},
isbn = {9781450382137},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3430524.3442460},
doi = {10.1145/3430524.3442460},
abstract = {In augmented reality (AR), gesture/hand-based interactions are becoming more common place over tangible user interfaces (TUIs) and physical controllers. Major concerns regard portability and battery-life with TUIs and physical controllers in an AR environment. For a TUI or physical controller to be usable ”on the go” and in the field, they need to be compact such that they can be easily deployable. Subsequently, it is desirable to be low-energy or powerless, such that the device remains functional after long periods. In this paper, we present our initial design towards a powerless, passive controller that leverages the optical camera of a AR head-mounted display (such as the Hololens or Magic Leap) for tracking. We discuss related work, design motivations, high and low fidelity designs, opportunistic/adaptable input modalities, and use cases. We conclude with propositions for future work.},
booktitle = {Proceedings of the Fifteenth International Conference on Tangible, Embedded, and Embodied Interaction},
articleno = {54},
numpages = {4},
keywords = {ephemeral interfaces, human-computer interaction, opportunistic controls, tangible user interfaces},
location = {Salzburg, Austria},
series = {TEI '21}
}

@inproceedings{Corsten_2013_InstantUI,
author = {Corsten, Christian and Avellino, Ignacio and M\"{o}llers, Max and Borchers, Jan},
title = {Instant user interfaces: repurposing everyday objects as input devices},
year = {2013},
isbn = {9781450322713},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2512349.2512799},
doi = {10.1145/2512349.2512799},
abstract = {Dedicated input devices are frequently used for system control. We present Instant User Interfaces, an interaction paradigm that loosens this dependency and allows operating a system even when its dedicated controller is unavailable. We implemented a reliable, marker-free object tracking system that enables users to assign semantic meaning to different poses or to touches in different areas. With this system, users can repurpose everyday objects and program them in an ad-hoc manner, using a GUI or by demonstration, as input devices. Users tested and ranked these methods alongside a Wizard-of-Oz speech interface. The testers did not show a clear preference as a group, but had individual preferences.},
booktitle = {Proceedings of the 2013 ACM International Conference on Interactive Tabletops and Surfaces},
pages = {71–80},
numpages = {10},
keywords = {instant uis, input devices, everyday objects, affordances},
location = {St. Andrews, Scotland, United Kingdom},
series = {ITS '13}
}

@inproceedings{Walsh_2014_EphemeralInteraction,
author = {Walsh, James A. and von Itzstein, Stewart and Thomas, Bruce H.},
title = {Ephemeral interaction using everyday objects},
year = {2014},
isbn = {9781921770333},
publisher = {Australian Computer Society, Inc.},
address = {AUS},
abstract = {The ability for Tangible User Interfaces to enable the intuitive control of existing systems and adapt to individual users' usage scenarios remains an area of development. Previous research in customizable tangible interfaces has focused primarily on the offline creation by the original system developer, instead of offering extensibility to the end user. This paper presents our system to support the ad-hoc creation of 'disposable' UIs using both projected controls and physical objects. To support these controls, a software based patch panel enables data to be mapped to external systems, and from external systems back to the system itself. Using a projector, depth camera and 6DOF tracking system, users can create and map tangible/touch-based ad-hoc user controls to existing system functionality. This allows users to both quickly create new inputs for existing functionality, as well as create new arbitrary input devices from completely passive components.},
booktitle = {Proceedings of the Fifteenth Australasian User Interface Conference - Volume 150},
pages = {29–37},
numpages = {9},
keywords = {ephemeral, extensible customizable, projected, reconfigurable, tangible, user interfaces},
location = {Auckland, New Zealand},
series = {AUIC '14}
}

@inproceedings{Sato_2012_Touche,
author = {Sato, Munehiko and Poupyrev, Ivan and Harrison, Chris},
title = {Touch\'{e}: enhancing touch interaction on humans, screens, liquids, and everyday objects},
year = {2012},
isbn = {9781450310154},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2207676.2207743},
doi = {10.1145/2207676.2207743},
abstract = {Touch\'{e} proposes a novel Swept Frequency Capacitive Sensing technique that can not only detect a touch event, but also recognize complex configurations of the human hands and body. Such contextual information significantly enhances touch interaction in a broad range of applications, from conventional touchscreens to unique contexts and materials. For example, in our explorations we add touch and gesture sensitivity to the human body and liquids. We demonstrate the rich capabilities of Touch\'{e} with five example setups from different application domains and conduct experimental studies that show gesture classification accuracies of 99\% are achievable with our technology.},
booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
pages = {483–492},
numpages = {10},
keywords = {gestures, mobile devices, on-body computing, sensors, touch, ubiquitous interfaces},
location = {Austin, Texas, USA},
series = {CHI '12}
}

@inproceedings{Dupre_2024_Tripad,
author = {Dupr\'{e}, Camille and Appert, Caroline and Rey, St\'{e}phanie and Saidi, Houssem and Pietriga, Emmanuel},
title = {TriPad: Touch Input in AR on Ordinary Surfaces with Hand Tracking Only},
year = {2024},
isbn = {9798400703300},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613904.3642323},
doi = {10.1145/3613904.3642323},
abstract = {TriPad enables opportunistic touch interaction in Augmented Reality using hand tracking only. Users declare the surface they want to appropriate with a simple hand tap gesture. They can then use this surface at will for direct and indirect touch input. TriPad only involves analyzing hand movements and postures, without the need for additional instrumentation, scene understanding or machine learning. TriPad thus works on a variety of flat surfaces, including glass. It also ensures low computational overhead on devices that typically have a limited power budget. We describe the approach, and report on two user studies. The first study demonstrates the robustness of TriPad’s hand movement interpreter on different surface materials. The second study compares TriPad against direct mid-air AR input techniques on both discrete and continuous tasks and with different surface orientations. TriPad achieves a better speed-accuracy trade-off overall, improves comfort and minimizes fatigue.},
booktitle = {Proceedings of the CHI Conference on Human Factors in Computing Systems},
articleno = {754},
numpages = {18},
keywords = {augmented reality, passive surfaces, touch input},
location = {Honolulu, HI, USA},
series = {CHI '24}
}

@inproceedings{Monteiro_2023_TeachableReality,
author = {Monteiro, Kyzyl and Vatsal, Ritik and Chulpongsatorn, Neil and Parnami, Aman and Suzuki, Ryo},
title = {Teachable Reality: Prototyping Tangible Augmented Reality with Everyday Objects by Leveraging Interactive Machine Teaching},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581449},
doi = {10.1145/3544548.3581449},
abstract = {This paper introduces Teachable Reality, an augmented reality (AR) prototyping tool for creating interactive tangible AR applications with arbitrary everyday objects. Teachable Reality leverages vision-based interactive machine teaching (e.g., Teachable Machine), which captures real-world interactions for AR prototyping. It identifies the user-defined tangible and gestural interactions using an on-demand computer vision model. Based on this, the user can easily create functional AR prototypes without programming, enabled by a trigger-action authoring interface. Therefore, our approach allows the flexibility, customizability, and generalizability of tangible AR applications that can address the limitation of current marker-based approaches. We explore the design space and demonstrate various AR prototypes, which include tangible and deformable interfaces, context-aware assistants, and body-driven AR applications. The results of our user study and expert interviews confirm that our approach can lower the barrier to creating functional AR prototypes while also allowing flexible and general-purpose prototyping experiences.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {459},
numpages = {15},
keywords = {Augmented Reality, Everyday Objects, Human-Centered Machine Learning;, Interactive Machine Teaching, Mixed Reality, Prototyping Tools, Tangible Interactions},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{Hettiarachchi_2016_AnnexingReality,
author = {Hettiarachchi, Anuruddha and Wigdor, Daniel},
title = {Annexing Reality: Enabling Opportunistic Use of Everyday Objects as Tangible Proxies in Augmented Reality},
year = {2016},
isbn = {9781450333627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2858036.2858134},
doi = {10.1145/2858036.2858134},
abstract = {Advances in display and tracking technologies hold the promise of increasingly immersive augmented-reality experiences. Unfortunately, the on-demand generation of haptic experiences is lagging behind these advances in other feedback channels. We present Annexing Reality; a system that opportunistically annexes physical objects from a user's current physical environment to provide the best-available haptic sensation for virtual objects. It allows content creators to a priori specify haptic experiences that adapt to the user's current setting. The system continuously scans user's surrounding, selects physical objects that are similar to given virtual objects, and overlays the virtual models on to selected physical ones reducing the visual-haptic mismatch. We describe the developer's experience with the Annexing Reality system and the techniques utilized in realizing it. We also present results of a developer study that validates the usability and utility of our method of defining haptic experiences.},
booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
pages = {1957–1967},
numpages = {11},
keywords = {augmented reality, augmented reality content authoring, opportunistic tangible interfaces},
location = {San Jose, California, USA},
series = {CHI '16}
}

@inproceedings{Jain_2023_UbiTouch,
author = {Jain, Rahul and Shi, Jingyu and Duan, Runlin and Zhu, Zhengzhe and Qian, Xun and Ramani, Karthik},
title = {Ubi-TOUCH: Ubiquitous Tangible Object Utilization through Consistent Hand-object interaction in Augmented Reality},
year = {2023},
isbn = {9798400701320},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3586183.3606793},
doi = {10.1145/3586183.3606793},
abstract = {Utilizing everyday objects as tangible proxies for Augmented Reality (AR) provides users with haptic feedback while interacting with virtual objects. Yet, existing methods focus on the attributes of the objects, constraining the possible proxies and yielding inconsistency in user experience. Therefore, we propose Ubi-TOUCH, an AR system that assists users in seeking a wider range of tangible proxies for AR applications based on the hand-object interaction (HOI) they desire. Given the target interaction with a virtual object, the system scans the users’ vicinity and recommends object proxies with similar interactions. Upon user selection, the system simultaneously tracks and maps users’ physical HOI to the virtual HOI, adaptively optimizing object 6 DoF and the hand gesture to provide consistency between the interactions. We showcase promising use cases of Ubi-TOUCH, such as remote tutorials, AR gaming, and Smart Home control. Finally, we evaluate the performance and usability of Ubi-TOUCH with a user study.},
booktitle = {Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology},
articleno = {12},
numpages = {18},
keywords = {Augmented Reality, Opportunistic Tangible Proxy, Tangible User Interface},
location = {San Francisco, CA, USA},
series = {UIST '23}
}

@inproceedings{He_2023_UbiEdge,
author = {He, Fengming and Hu, Xiyun and Shi, Jingyu and Qian, Xun and Wang, Tianyi and Ramani, Karthik},
title = {Ubi Edge: Authoring Edge-Based Opportunistic Tangible User Interfaces in Augmented Reality},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3580704},
doi = {10.1145/3544548.3580704},
abstract = {Edges are one of the most ubiquitous geometric features of physical objects. They provide accurate haptic feedback and easy-to-track features for camera systems, making them an ideal basis for Tangible User Interfaces (TUI) in Augmented Reality (AR). We introduce Ubi Edge, an AR authoring tool that allows end-users to customize edges on daily objects as TUI inputs to control varied digital functions. We develop an integrated AR-device and an integrated vision-based detection pipeline that can track 3D edges and detect the touch interaction between fingers and edges. Leveraging the spatial-awareness of AR, users can simply select an edge by sliding fingers along it and then make the edge interactive by connecting it to various digital functions. We demonstrate four use cases including multi-function controllers, smart homes, games, and TUI-based tutorials. We also evaluated and proved our system’s usability through a two-session user study, where qualitative and quantitative results are positive.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {461},
numpages = {14},
keywords = {Augmented Reality, Tangible User Interface, immersive authoring},
location = {Hamburg, Germany},
series = {CHI '23}
}

@inproceedings{Zhang_2019_ActiTouch,
author = {Zhang, Yang and Kienzle, Wolf and Ma, Yanjun and Ng, Shiu S. and Benko, Hrvoje and Harrison, Chris},
title = {ActiTouch: Robust Touch Detection for On-Skin AR/VR Interfaces},
year = {2019},
isbn = {9781450368162},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3332165.3347869},
doi = {10.1145/3332165.3347869},
abstract = {Contemporary AR/VR systems use in-air gestures or handheld controllers for interactivity. This overlooks the skin as a convenient surface for tactile, touch-driven interactions, which are generally more accurate and comfortable than free space interactions. In response, we developed ActiTouch, a new electrical method that enables precise on-skin touch segmentation by using the body as an RF waveguide. We combine this method with computer vision, enabling a system with both high tracking precision and robust touch detection. Our system requires no cumbersome instrumentation of the fingers or hands, requiring only a single wristband (e.g., smartwatch) and sensors integrated into an AR/VR headset. We quantify the accuracy of our approach through a user study and demonstrate how it can enable touchscreen-like interactions on the skin.},
booktitle = {Proceedings of the 32nd Annual ACM Symposium on User Interface Software and Technology},
pages = {1151–1159},
numpages = {9},
keywords = {augmented reality (ar), finger tracking, touch interaction, touch segmentation, virtual reality (vr)},
location = {New Orleans, LA, USA},
series = {UIST '19}
}

@inproceedings{Harrison_2011_OmniTouch,
author = {Harrison, Chris and Benko, Hrvoje and Wilson, Andrew D.},
title = {OmniTouch: wearable multitouch interaction everywhere},
year = {2011},
isbn = {9781450307161},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2047196.2047255},
doi = {10.1145/2047196.2047255},
abstract = {OmniTouch is a wearable depth-sensing and projection system that enables interactive multitouch applications on everyday surfaces. Beyond the shoulder-worn system, there is no instrumentation of the user or environment. Foremost, the system allows the wearer to use their hands, arms and legs as graphical, interactive surfaces. Users can also transiently appropriate surfaces from the environment to expand the interactive area (e.g., books, walls, tables). On such surfaces - without any calibration - OmniTouch provides capabilities similar to that of a mouse or touchscreen: X and Y location in 2D interfaces and whether fingers are "clicked" or hovering, enabling a wide variety of interactions. Reliable operation on the hands, for example, requires buttons to be 2.3cm in diameter. Thus, it is now conceivable that anything one can do on today's mobile devices, they could do in the palm of their hand.},
booktitle = {Proceedings of the 24th Annual ACM Symposium on User Interface Software and Technology},
pages = {441–450},
numpages = {10},
keywords = {on-demand interfaces, on-body computing, object classification, finger tracking, appropriated surfaces},
location = {Santa Barbara, California, USA},
series = {UIST '11}
}

@inproceedings{Zhou_2020_Gripmarks,
author = {Zhou, Qian and Sykes, Sarah and Fels, Sidney and Kin, Kenrick},
title = {Gripmarks: Using Hand Grips to Transform In-Hand Objects into Mixed Reality Input},
year = {2020},
isbn = {9781450367080},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3313831.3376313},
doi = {10.1145/3313831.3376313},
abstract = {We introduce Gripmarks, a system that enables users to opportunistically use objects they are already holding as input surfaces for mixed reality head-mounted displays (HMD). Leveraging handheld objects reduces the need for users to free up their hands or acquire a controller to interact with their HMD. Gripmarks associate a particular hand grip with the shape primitive of the physical object without the need of object recognition or instrumenting the object. From the grip pose and shape primitive we can infer the surface of the object. With an activation gesture, we can enable the object for use as input to the HMD. With five gripmarks we demonstrate a recognition rate of 94.2\%; we show that our grip detection benefits from the physical constraints of holding an object. We explore two categories of input objects 1) tangible surfaces and 2) tangible tools and present two representative applications. We discuss the design and technical challenges for expanding the concept.},
booktitle = {Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–11},
numpages = {11},
keywords = {grip recognition, gripmarks, mixed reality, tangible objects},
location = {Honolulu, HI, USA},
series = {CHI '20}
}

@inproceedings{Kim_HoVR_2016,
author = {Kim, Youngwon R. and Kim, Gerard J.},
title = {HoVR-type: smartphone as a typing interface in VR using hovering},
year = {2016},
isbn = {9781450344913},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2993369.2996330},
doi = {10.1145/2993369.2996330},
abstract = {We propose a text entry method for VR, using the smartphone and its hovering function, called the HoVR-Type. The hovering function effectively acts as the finger tracking sensor thereby allowing the user to type in the virtual space. When added with the additional phase to correct the initial touch input and having the final key entered upon the finger release, the proposed method showed competitive performance to that of the conventional "aim-and-shoot" method and also exhibited much higher usability. Overall, the use of the smartphone leverages on the already established mobile user experience and can be further extended to other VR interaction techniques so that one can use the common smartphone as an all-purpose VR interaction device.},
booktitle = {Proceedings of the 22nd ACM Conference on Virtual Reality Software and Technology},
pages = {333–334},
numpages = {2},
keywords = {touch input, text entry, smartphone interaction, hovering, VR input},
location = {Munich, Germany},
series = {VRST '16}
}

@ARTICLE{Grubert_2024_TextEntry,
  author={Grubert, Jens and Witzani, Lukas and Otte, Alexander and Gesslein, Travis and Kranz, Matthias and Kristensson, Per Ola},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Text Entry Performance and Situation Awareness of a Joint Optical See-Through Head-Mounted Display and Smartphone System}, 
  year={2024},
  volume={30},
  number={8},
  pages={5830-5846},
  abstract={Optical see-through head-mounted displays (OST HMDs) are a popular output medium for mobile Augmented Reality (AR) applications. To date, they lack efficient text entry techniques. Smartphones are a major text entry medium in mobile contexts but attentional demands can contribute to accidents while typing on the go. Mobile multi-display ecologies, such as combined OST HMD-smartphone systems, promise performance and situation awareness benefits over single-device use. We study the joint performance of text entry on mobile phones with text output on optical see-through head-mounted displays. A series of five experiments with a total of 86 participants indicate that, as of today, the challenges in such a joint interactive system outweigh the potential benefits.},
  keywords={Resists;Keyboards;Switches;Visualization;Head-mounted displays;Task analysis;Standards;Augmented reality;cross-device;head-mounted display;mobile;multi-display;optical see-through;text entry},
  doi={10.1109/TVCG.2023.3309316},
  ISSN={1941-0506},
  month={Aug},}

@INPROCEEDINGS{Boustila_2019_TextTyping,
  author={Boustila, Sabah and Guégan, Thomas and Takashima, Kazuki and Kitamura, Yoshifumi},
  booktitle={2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)}, 
  title={Text Typing in VR Using Smartphones Touchscreen and HMD}, 
  year={2019},
  volume={},
  number={},
  pages={860-861},
  keywords={Smart phones;Keyboards;Error analysis;Resists;Virtual reality;Prototypes;Writing;Human-centered computing—Virtual reality—Touch screens;Human-centered computing—User interface design},
  doi={10.1109/VR.2019.8798238}}


@InProceedings{Hoppe_2018,
author="Hoppe, Adrian H.
and Otto, Leonard
and van de Camp, Florian
and Stiefelhagen, Rainer
and Unm{\"u}{\ss}ig, Gabriel",
editor="Stephanidis, Constantine",
title="qVRty: Virtual Keyboard with a Haptic, Real-World Representation",
booktitle="HCI International 2018 -- Posters' Extended Abstracts",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="266--272",
abstract="Virtual Reality systems offer great possibilities to analyze and interact with data. However, they still lack a commonly accepted, efficient text input technique that allows users to record their findings. To provide users with an efficient technique for text input, a real keyboard and the user's hands are transferred into the virtual world. This allows real haptic feedback of the device and, as a user study shows, results in fast and accurate text writing. The proposed approach shows that a real-world ability can be transmitted directly into the virtual world without much loss.",
isbn="978-3-319-92279-9"
}
@inproceedings{Ishii1997TangibleBits,
author = {Ishii, Hiroshi and Ullmer, Brygg},
title = {Tangible bits: towards seamless interfaces between people, bits and atoms},
year = {1997},
isbn = {0897918029},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/258549.258715},
doi = {10.1145/258549.258715},
booktitle = {Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems},
pages = {234–241},
numpages = {8},
keywords = {ambient media, augmented reality, center and periphery, foreground and background, graspable user interface, tangible user interface, ubiquitous computing},
location = {Atlanta, Georgia, USA},
series = {CHI '97}
}

@inproceedings{Yi_2015_ATK,
author = {Yi, Xin and Yu, Chun and Zhang, Mingrui and Gao, Sida and Sun, Ke and Shi, Yuanchun},
title = {ATK: Enabling Ten-Finger Freehand Typing in Air Based on 3D Hand Tracking Data},
year = {2015},
isbn = {9781450337793},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2807442.2807504},
doi = {10.1145/2807442.2807504},
abstract = {Ten-finger freehand mid-air typing is a potential solution for post-desktop interaction. However, the absence of tactile feedback as well as the inability to accurately distinguish tapping finger or target keys exists as the major challenge for mid-air typing. In this paper, we present ATK, a novel interaction technique that enables freehand ten-finger typing in the air based on 3D hand tracking data. Our hypothesis is that expert typists are able to transfer their typing ability from physical keyboards to mid-air typing. We followed an iterative approach in designing ATK. We first empirically investigated users' mid-air typing behavior, and examined fingertip kinematics during tapping, correlated movement among fingers and 3D distribution of tapping endpoints. Based on the findings, we proposed a probabilistic tap detection algorithm, and augmented Goodman's input correction model to account for the ambiguity in distinguishing tapping finger. We finally evaluated the performance of ATK with a 4-block study. Participants typed 23.0 WPM with an uncorrected word-level error rate of 0.3\% in the first block, and later achieved 29.2 WPM in the last block without sacrificing accuracy.},
booktitle = {Proceedings of the 28th Annual ACM Symposium on User Interface Software \& Technology},
pages = {539–548},
numpages = {10},
keywords = {mid-air interaction, text entry, user interface, word disambiguation},
location = {Charlotte, NC, USA},
series = {UIST '15}
}
@inproceedings{Singhal_2022_StrokeRecognition,
author = {Singhal, Yatharth and Noeske, Richard Huynh and Bhardwaj, Ayush and Kim, Jin Ryong},
title = {Improving Finger Stroke Recognition Rate for Eyes-Free Mid-Air Typing in VR},
year = {2022},
isbn = {9781450391573},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3491102.3502100},
doi = {10.1145/3491102.3502100},
abstract = {We examine mid-air typing data collected from touch typists to evaluate the features and classification models for recognizing finger stroke. A large number of finger movement traces have been collected using finger motion capture systems, labeled into individual finger strokes, and classified into several key features. We test finger kinematic features, including 3D position, velocity, acceleration, and temporal features, including previous fingers and keys. Based on this analysis, we assess the performance of various classifiers, including Naive Bayes, Random Forest, Support Vector Machines, and Deep Neural Networks, in terms of the accuracy for correctly classifying the keystroke. We finally incorporate a linguistic heuristic to explore the effectiveness of the character prediction model and improve the total accuracy.},
booktitle = {Proceedings of the 2022 CHI Conference on Human Factors in Computing Systems},
articleno = {346},
numpages = {9},
keywords = {Text Entry, Mid-Air Typing, Keystroke Classification, Eyes-free Typing, Deep Neural Network},
location = {New Orleans, LA, USA},
series = {CHI '22}
}

@inproceedings{matias1993half,
  title={Half-QWERTY: A one-handed keyboard facilitating skill transfer from QWERTY},
  author={Matias, Edgar and MacKenzie, I Scott and Buxton, William},
  booktitle={Proceedings of the INTERACT'93 and CHI'93 Conference on Human Factors in Computing Systems},
  pages={88--94},
  year={1993}
}

@article{Gil_2023_ThumbAir,
author = {Gil, Hyunjae and Oakley, Ian},
title = {ThumbAir: In-Air Typing for Head Mounted Displays},
year = {2023},
issue_date = {December 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {6},
number = {4},
url = {https://doi.org/10.1145/3569474},
doi = {10.1145/3569474},
abstract = {Typing while wearing a standalone Head Mounted Display (HMD)---systems without external input devices or sensors to support text entry---is hard. To address this issue, prior work has used external trackers to monitor finger movements to support in-air typing on virtual keyboards. While performance has been promising, current systems are practically infeasible: finger movements may be visually occluded from inside-out HMD based tracking systems or, otherwise, awkward and uncomfortable to perform. To address these issues, this paper explores an alternative approach. Taking inspiration from the prevalence of thumb-typing on mobile phones, we describe four studies exploring, defining and validating the performance of ThumbAir, an in-air thumb-typing system implemented on a commercial HMD. The first study explores viable target locations, ultimately recommending eight targets sites. The second study collects performance data for taps on pairs of these targets to both inform the design of a target selection procedure and also support a computational design process to select a keyboard layout. The final two studies validate the selected keyboard layout in word repetition and phrase entry tasks, ultimately achieving final WPMs of 27.1 and 13.73. Qualitative data captured in the final study indicate that the discreet movements required to operate ThumbAir, in comparison to the larger scale finger and hand motions used in a baseline design from prior work, lead to reduced levels of perceived exertion and physical demand and are rated as acceptable for use in a wider range of social situations.},
journal = {Proc. ACM Interact. Mob. Wearable Ubiquitous Technol.},
month = {jan},
articleno = {164},
numpages = {30},
keywords = {Head Mounted Display, Text entry, Virtual Reality}
}

@inproceedings{Vatavu_2015_AgreementRate,
author = {Vatavu, Radu-Daniel and Wobbrock, Jacob O.},
title = {Formalizing Agreement Analysis for Elicitation Studies: New Measures, Significance Test, and Toolkit},
year = {2015},
isbn = {9781450331456},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2702123.2702223},
doi = {10.1145/2702123.2702223},
abstract = {We address in this work the process of agreement rate analysis for characterizing the level of consensus between participants' proposals elicited during guessability studies. Two new measures, i.e., disagreement rate for referents and coagreement rate between referents, are proposed to accompany the widely-used agreement rate formula of Wobbrock et al. [37] when reporting participants' consensus for symbolic input. A statistical significance test for comparing the agreement rates of k>=2 referents is presented in analogy with Cochran's success/failure Q test [5], for which we express the test statistic in terms of agreement and coagreement rates. We deliver a toolkit to assist practitioners to compute agreement, disagreement, and coagreement rates, and run statistical tests for agreement rates at p=.05, .01, and .001 levels of significance. We validate our theoretical development of agreement rate analysis in relation with several previously published elicitation studies. For example, when we present the probability distribution function of the agreement rate measure, we also use it (1) to explain the magnitude of agreement rates previously reported in the literature, and (2) to propose qualitative interpretations for agreement rates, in analogy with Cohen's guidelines for effect sizes [6]. We also re-examine previously published elicitation data from the perspective of the agreement rate test statistic, and highlight new findings on the effect of referents over agreement rates, unattainable prior to this work. We hope that our contributions will advance the current knowledge in agreement rate analysis, providing researchers and practitioners with new techniques and tools to help them understand user-elicited data at deeper levels of detail and sophistication.},
booktitle = {Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems},
pages = {1325–1334},
numpages = {10},
keywords = {agreement rate, coagreement, disagreement, guessability study, methodology, statistical test, user-defined gestures},
location = {Seoul, Republic of Korea},
series = {CHI '15}
}

@inproceedings{MacKenzie_2003_Phrases,
author = {MacKenzie, I. Scott and Soukoreff, R. William},
title = {Phrase sets for evaluating text entry techniques},
year = {2003},
isbn = {1581136374},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/765891.765971},
doi = {10.1145/765891.765971},
abstract = {In evaluations of text entry methods, participants enter phrases of text using a technique of interest while performance data are collected. This paper describes and publishes (via the internet) a collection of 500 phrases for such evaluations. Utility programs are also provided to compute statistical properties of the phrase set, or any other phrase set. The merits of using a pre-defined phrase set are described as are methodological considerations, such as attaining results that are generalizable and the possible addition of punctuation and other characters.},
booktitle = {CHI '03 Extended Abstracts on Human Factors in Computing Systems},
pages = {754–755},
numpages = {2},
keywords = {pen and tactile input, input and interaction technologies, hand held devices and mobile computing},
location = {Ft. Lauderdale, Florida, USA},
series = {CHI EA '03}
}

@inproceedings{Tang_2020_GrabAR,
author = {Tang, Xiao and Hu, Xiaowei and Fu, Chi-Wing and Cohen-Or, Daniel},
title = {GrabAR: Occlusion-aware Grabbing Virtual Objects in AR},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415835},
doi = {10.1145/3379337.3415835},
abstract = {Existing augmented reality (AR) applications often ignore the occlusion between real hands and virtual objects when incorporating virtual objects in user's views. The challenges come from the lack of accurate depth and mismatch between real and virtual depth. This paper presents GrabAR1, a new approach that directly predicts the real-and-virtual occlusion and bypasses the depth acquisition and inference. Our goal is to enhance AR applications with interactions between hand (real) and grabbable objects (virtual). With paired images of hand and object as inputs, we formulate a compact deep neural network that learns to generate the occlusion mask. To train the network, we compile a large dataset, including synthetic data and real data. We then embed the trained network in a prototyping AR system to support real-time grabbing of virtual objects. Further, we demonstrate the performance of our method on various virtual objects, compare our method with others through two user studies, and showcase a rich variety of interaction scenarios, in which we can use bare hand to grab virtual objects and directly manipulate them.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {697–708},
numpages = {12},
keywords = {occlusion, neural network, interaction, augmented reality},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{Fashimpaur_et_al_2020,
author = {Fashimpaur, Jacqui and Kin, Kenrick and Longest, Matt},
title = {PinchType: Text Entry for Virtual and Augmented Reality Using Comfortable Thumb to Fingertip Pinches},
year = {2020},
isbn = {9781450368193},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3334480.3382888},
doi = {10.1145/3334480.3382888},
abstract = {Text entry is an integral component to many use cases in virtual and augmented reality. We present PinchType: A new method of virtual text entry that combines users' existing knowledge of the QWERTY keyboard layout with simple thumb and finger interactions. Users pinch with the thumb and fingertip to select from the same group of letters the finger would press on a QWERTY keyboard; a language model disambiguates. In a preliminary study with 14 participants, we investigated PinchType's speed and accuracy on initial use, as well as its physical comfort relative to a mid-air keyboard. After entering 40 phrases, most people reported that PinchType was more comfortable than the mid-air keyboard. Most participants reached a mean speed of 12.54 WPM, or 20.07 WPM without the time spent correcting errors. This compares favorably to other thumb-to-finger virtual text entry methods.},
booktitle = {Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems},
pages = {1–7},
numpages = {7},
keywords = {mixed reality, pinch typing, text entry},
location = {<conf-loc>, <city>Honolulu</city>, <state>HI</state>, <country>USA</country>, </conf-loc>},
series = {CHI EA '20}
}

@inproceedings{Wong_et_al_2018,
author = {Wong, Pui Chung and Zhu, Kening and Fu, Hongbo},
title = {FingerT9: Leveraging Thumb-to-finger Interaction for Same-side-hand Text Entry on Smartwatches},
year = {2018},
isbn = {9781450356206},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3173574.3173752},
doi = {10.1145/3173574.3173752},
abstract = {We introduce FingerT9, leveraging the action of thumb-to-finger touching on the finger segments, to support same-side-hand (SSH) text entry on smartwatches. This is achieved by mapping a T9 keyboard layout to the finger segments. Our solution avoids the problems of fat finger and screen occlusion, and enables text entry using the same-side hand which wears the watch. In the pilot study, we determined the layout mapping preferred by the users. We conducted an experiment to compare the text-entry performances of FingerT9, the tilt-based SSH input, and the direct-touch non-SSH input. The results showed that the participants performed significantly faster and more accurately with FingerT9 than the tilt-based method. There was no significant difference between FingerT9 and direct-touch methods in terms of efficiency and error rate. We then conducted the second experiment to study the learning curve on SSH text entry methods: FingerT9 and the tilt-based input. FingerT9 gave significantly better long-term improvement. In addition, eyes-free text entry (i.e., looking at the screen output but not the keyboard layout mapped on the finger segments) was made possible once the participants were familiar with the keyboard layout.},
booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
pages = {1–10},
numpages = {10},
keywords = {mobile interaction, same-sided hand interaction, smartwatch, text entry, thumb-to-finger interaction},
location = {<conf-loc>, <city>Montreal QC</city>, <country>Canada</country>, </conf-loc>},
series = {CHI '18}
}

@inproceedings{Gong_2023_AffordanceBased,
author = {Gong, Weilun and Santosa, Stephanie and Grossman, Tovi and Glueck, Michael and Clarke, Daniel and Lai, Frances},
title = {Affordance-Based and User-Defined Gestures for Spatial Tangible Interaction},
year = {2023},
isbn = {9781450398930},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3563657.3596032},
doi = {10.1145/3563657.3596032},
abstract = {Although mid-air hand gestures have been widely adopted by VR/AR products (e.g., Quest 2 and HoloLens), some drawbacks remain due to their lack of tangibility and tactile feedback. Opportunistic Tangible User Interfaces could address these shortcomings by repurposing existing objects in one's physical environment. However, there has yet to be a systematic investigation of the gestures that would be desirable when using opportunistic objects or how such gestures would be impacted by such objects. In this work, we conducted an elicitation study to investigate the desirability of object and gesture combinations across a variety of interactions. The results contribute (1) an opportunistic tangible UI gesture set for spatial interfaces, and (2) an Affordance-Based Object Selector Scheme that identifies ideal objects for tangible input given a desired input gesture, based on that object's physical affordances. Arising from these findings is the vision of the Adaptive Tangible User Interface, which supports the on-the-fly composition of tangible interfaces based on the affordances found in the physical environment and a user's input task.},
booktitle = {Proceedings of the 2023 ACM Designing Interactive Systems Conference},
pages = {1500–1514},
numpages = {15},
keywords = {Embodied Interaction, Input Technique, Tangible User Interface},
location = {Pittsburgh, PA, USA},
series = {DIS '23}
}

@inproceedings{Ahlstrom_2014_AroundDeviceGesture,
author = {Ahlstr\"{o}m, David and Hasan, Khalad and Irani, Pourang},
title = {Are you comfortable doing that? acceptance studies of around-device gestures in and for public settings},
year = {2014},
isbn = {9781450330046},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2628363.2628381},
doi = {10.1145/2628363.2628381},
abstract = {Several research groups have demonstrated advantages of extending a mobile device's input vocabulary with in-air gestures. Such gestures show promise but are not yet being integrated onto commercial devices. One reason for this might be the uncertainty about users' perceptions regarding the social acceptance of such around-device gestures. In three studies, performed in public settings, we explore users' and spectators' attitudes about using around-device gestures in public. The results show that people are concerned about others' reactions. They are also sensitive and selective regarding where and in front of whom they would feel comfortable using around-device gestures. However, acceptance and comfort are strongly linked to gesture characteristics, such as, gesture size, duration and in-air position. Based on our findings we present recommendations for around-device input designers and suggest new approaches for evaluating the social acceptability of novel input methods.},
booktitle = {Proceedings of the 16th International Conference on Human-Computer Interaction with Mobile Devices \& Services},
pages = {193–202},
numpages = {10},
keywords = {around-device input, gesture design, user acceptance},
location = {Toronto, ON, Canada},
series = {MobileHCI '14}
}

@inproceedings{Wang_2020_CAPturAR,
author = {Wang, Tianyi and Qian, Xun and He, Fengming and Hu, Xiyun and Huo, Ke and Cao, Yuanzhi and Ramani, Karthik},
title = {CAPturAR: An Augmented Reality Tool for Authoring Human-Involved Context-Aware Applications},
year = {2020},
isbn = {9781450375146},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3379337.3415815},
doi = {10.1145/3379337.3415815},
abstract = {Recognition of human behavior plays an important role in context-aware applications. However, it is still a challenge for end-users to build personalized applications that accurately recognize their own activities. Therefore, we present CAPturAR, an in-situ programming tool that supports users to rapidly author context-aware applications by referring to their previous activities. We customize an AR head-mounted device with multiple camera systems that allow for non-intrusive capturing of user's daily activities. During authoring, we reconstruct the captured data in AR with an animated avatar and use virtual icons to represent the surrounding environment. With our visual programming interface, users create human-centered rules for the applications and experience them instantly in AR. We further demonstrate four use cases enabled by CAPturAR. Also, we verify the effectiveness of the AR-HMD and the authoring workflow with a system evaluation using our prototype. Moreover, we conduct a remote user study in an AR simulator to evaluate the usability.},
booktitle = {Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology},
pages = {328–341},
numpages = {14},
keywords = {augmented reality, context-aware application, embodied authoring, end-user programming tool, in-situ authoring, ubiquitous computing},
location = {Virtual Event, USA},
series = {UIST '20}
}

@inproceedings{Vanacken_2008_DesigningMultimodalVE,
author = {Vanacken, Lode and De Boeck, Joan and Raymaekers, Chris and Coninx, Karin},
title = {Designing context-aware multimodal virtual environments},
year = {2008},
isbn = {9781605581989},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1452392.1452418},
doi = {10.1145/1452392.1452418},
abstract = {Despite of decades of research, creating intuitive and easy to learn interfaces for 3D virtual environments (VE) is still not obvious, requiring VE specialists to define, implement and evaluate solutions in an iterative way, often using low-level programming code. Moreover, quite frequently the interaction with the virtual environment may also vary dependent on the context in which it is applied, such as the available hardware setup, user experience, or the pose of the user (e.g. sitting or standing). Lacking other tools, the context-awareness of an application is usually implemented in an ad-hoc manner, using low-level programming, as well. This may result in code that is difficult and expensive to maintain. One possible approach to facilitate the process of creating these highly interactive user interfaces is by adopting a model-based user interface design. This lifts the creation of a user interface to a higher level allowing the designer to reason more in terms of high-level concepts, rather than writing programming code. In this paper, we adopt a model-based user interface design (MBUID) process for the creation of VEs, and explain how a context system using an Event-Condition-Action paradigm is added. We illustrate our approach by means of a case study.},
booktitle = {Proceedings of the 10th International Conference on Multimodal Interfaces},
pages = {129–136},
numpages = {8},
keywords = {context-awareness, model-based user interface design, multimodal interaction techniques},
location = {Chania, Crete, Greece},
series = {ICMI '08}
}

@inproceedings{Yu_2017_TapDwellGesture,
author = {Yu, Chun and Gu, Yizheng and Yang, Zhican and Yi, Xin and Luo, Hengliang and Shi, Yuanchun},
title = {Tap, Dwell or Gesture? Exploring Head-Based Text Entry Techniques for HMDs},
year = {2017},
isbn = {9781450346559},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3025453.3025964},
doi = {10.1145/3025453.3025964},
abstract = {Despite the increasing popularity of head mounted displays (HMDs), development of efficient text entry methods on these devices has remained under explored. In this paper, we investigate the feasibility of head-based text entry for HMDs, by which, the user controls a pointer on a virtual keyboard using head rotation. Specifically, we investigate three techniques: TapType, DwellType, and GestureType. Users of TapType select a letter by pointing to it and tapping a button. Users of DwellType select a letter by pointing to it and dwelling over it for a period of time. Users of GestureType perform word-level input using a gesture typing style. Two lab studies were conducted. In the first study, users typed 10.59 WPM, 15.58 WPM, and 19.04 WPM with DwellType, TapType, and GestureType, respectively. Users subjectively felt that all three of the techniques were easy to learn and considered the induced fatigue to be acceptable. In the second study, we further investigated GestureType. We improved its gesture-word recognition algorithm by incorporating the head movement pattern obtained from the first study. This resulted in users reaching 24.73 WPM after 60 minutes of training. Based on these results, we argue that head-based text entry is feasible and practical on HMDs, and deserves more attention.},
booktitle = {Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems},
pages = {4479–4488},
numpages = {10},
keywords = {dwelling, gesture keyboard, head-based text entry, hmd},
location = {Denver, Colorado, USA},
series = {CHI '17}
}

@inproceedings{Gil_2020_Characterizing,
author = {Gil, Hyunjae and Shin, Yonghwan and Son, Hyungki and Hwang, Inwook and Oakley, Ian and Kim, Jin Ryong},
title = {Characterizing In-Air Eyes-Free Typing Movements in VR},
year = {2020},
isbn = {9781450376198},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3385956.3418963},
doi = {10.1145/3385956.3418963},
abstract = {We empirically explore fundamental requirements for achieving VR in-air typing by observing the unconstrained eyes-free in-air typing of touch typists. We show that unconstrained typing movements differ substantively from previously observed constrained in-air typing movements and introduce a novel binary categorization of typing strategies: typists who use finger movements alone (FINGER) and those who combine finger movement with gross hand movement (HAND). We examine properties of finger kinematics, correlated movement of fingers, interrelation in consecutive key-strokes, and 3D distribution of key-stroke movements. We report that, compared to constrained typing, unconstrained typing generates shorter (49 mm) and faster (764 mm/s) key-strokes with a high correlation of finger movement and that the HAND strategy group exhibits more dynamic key-strokes. We discuss how these findings can inform the design of future in-air typing systems.},
booktitle = {Proceedings of the 26th ACM Symposium on Virtual Reality Software and Technology},
articleno = {7},
numpages = {10},
keywords = {In-Air Typing, Text Entry, Typing in VR},
location = {Virtual Event, Canada},
series = {VRST '20}
}

@ARTICLE{Luong_2023_ControllersorBareHands,
  author={Luong, Tiffany and Cheng, Yi Fei and Möbus, Max and Fender, Andreas and Holz, Christian},
  journal={IEEE Transactions on Visualization and Computer Graphics}, 
  title={Controllers or Bare Hands? A Controlled Evaluation of Input Techniques on Interaction Performance and Exertion in Virtual Reality}, 
  year={2023},
  volume={29},
  number={11},
  pages={4633-4643},
  keywords={Task analysis;Behavioral sciences;Tracking;Measurement;Ergonomics;Trajectory;Reliability;Virtual Reality;Input Modality;Controllers;Hand Tracking;Raycast;Touch;Physical Exertion;Performance;Behavior},
  doi={10.1109/TVCG.2023.3320211}}
